import torch
import cv2
import mediapipe as mp

# âœ… Mediapipe ì´ˆê¸°í™”
mp_face_detection = mp.solutions.face_detection
mp_face_mesh = mp.solutions.face_mesh

# âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
old_data_path = "merged_data_celeba.pt"  # ê¸°ì¡´ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
new_data_path = "merged_data_celeba2.pt"  # ìƒˆë¡œ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
data = torch.load(old_data_path, map_location="cpu")

# âœ… ìƒˆë¡œìš´ ë°ì´í„° ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
new_data = []

# âœ… BlazeFace & FaceMesh ëª¨ë¸ ì´ˆê¸°í™”
with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection, \
     mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5) as face_mesh:

    # âœ… ê¸°ì¡´ ë°ì´í„° ìˆœíšŒ
    for i, (image_path, _, _) in enumerate(data):
        image = cv2.imread(image_path)
        if image is None:
            print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
            continue

        h, w, _ = image.shape
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # OpenCVëŠ” BGR, MediapipeëŠ” RGB ì‚¬ìš©

        # ğŸ”¹ 1ï¸âƒ£ BlazeFaceë¥¼ ì‚¬ìš©í•˜ì—¬ BBox ì¶”ì¶œ
        face_results = face_detection.process(image_rgb)
        if not face_results.detections:
            print(f"âŒ ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨: {image_path}")
            continue

        detection = face_results.detections[0]  # ì²« ë²ˆì§¸ ì–¼êµ´ë§Œ ì‚¬ìš©
        bboxC = detection.location_data.relative_bounding_box
        x1 = int(bboxC.xmin * w)
        y1 = int(bboxC.ymin * h)
        x2 = x1 + int(bboxC.width * w)
        y2 = y1 + int(bboxC.height * h)
        bbox = [x1, y1, x2, y2]  # âœ… (x1, y1, x2, y2) í”½ì…€ ë‹¨ìœ„ ìœ ì§€

        # ğŸ”¹ 2ï¸âƒ£ FaceMeshë¥¼ ì‚¬ìš©í•˜ì—¬ ëœë“œë§ˆí¬ ì¶”ì¶œ
        face_mesh_results = face_mesh.process(image_rgb)
        if not face_mesh_results.multi_face_landmarks:
            print(f"âŒ ëœë“œë§ˆí¬ ê²€ì¶œ ì‹¤íŒ¨: {image_path}")
            continue

        face_landmarks = face_mesh_results.multi_face_landmarks[0]

        # ğŸ”¹ 3ï¸âƒ£ ìƒˆë¡œìš´ ëœë“œë§ˆí¬ êµ¬ì„± (7ê°œ í¬ì¸íŠ¸, 0~1 ì •ê·œí™”ëœ ê°’)
        landmarks = []
        key_indices = {
            "ì˜¤ë¥¸ìª½ ìœ„ ëˆˆêº¼í’€": 159,
            "ì˜¤ë¥¸ìª½ ì•„ë˜ ëˆˆêº¼í’€": 145,
            "ì™¼ìª½ ìœ„ ëˆˆêº¼í’€": 386,
            "ì™¼ìª½ ì•„ë˜ ëˆˆêº¼í’€": 374,
            "ì½”": 1,
            "ì˜¤ë¥¸ìª½ ì…ê¼¬ë¦¬": 61,
            "ì™¼ìª½ ì…ê¼¬ë¦¬": 291,
            "ìœ—ì…ìˆ ": 0,
            "ì•„ë«ì…ìˆ ": 17,
        }

        # ğŸ”¹ ì˜¤ë¥¸ìª½ ëˆˆ ì¤‘ì•™ ê³„ì‚° (ì •ê·œí™”ëœ ê°’)
        right_eye_x = (face_landmarks.landmark[159].x + face_landmarks.landmark[145].x) / 2
        right_eye_y = (face_landmarks.landmark[159].y + face_landmarks.landmark[145].y) / 2
        landmarks.extend([right_eye_x, right_eye_y])

        # ğŸ”¹ ì™¼ìª½ ëˆˆ ì¤‘ì•™ ê³„ì‚° (ì •ê·œí™”ëœ ê°’)
        left_eye_x = (face_landmarks.landmark[386].x + face_landmarks.landmark[374].x) / 2
        left_eye_y = (face_landmarks.landmark[386].y + face_landmarks.landmark[374].y) / 2
        landmarks.extend([left_eye_x, left_eye_y])

        # ğŸ”¹ ë‚˜ë¨¸ì§€ ëœë“œë§ˆí¬ ì¶”ê°€ (ì •ê·œí™”ëœ ê°’)
        for key in ["ì½”", "ì˜¤ë¥¸ìª½ ì…ê¼¬ë¦¬", "ì™¼ìª½ ì…ê¼¬ë¦¬", "ìœ—ì…ìˆ ", "ì•„ë«ì…ìˆ "]:
            idx = key_indices[key]
            x, y = face_landmarks.landmark[idx].x, face_landmarks.landmark[idx].y
            landmarks.extend([x, y])

        # ğŸ”¹ 4ï¸âƒ£ ìƒˆë¡œìš´ ë°ì´í„° ì €ì¥ (âœ… ê¸°ì¡´ í˜•ì‹ ìœ ì§€)
        new_data.append((image_path, bbox, landmarks))

        # ğŸ”¹ ì§„í–‰ ìƒí™© ì¶œë ¥
        if i % 500 == 0:
            print(f"ğŸ”„ ì§„í–‰ ì¤‘: {i}/{len(data)}")
       
# âœ… ìµœì¢… ë°ì´í„° ì €ì¥
torch.save(new_data, new_data_path)
print(f"âœ… ìƒˆë¡œìš´ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {new_data_path}")
