import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import os
from PIL import Image
from tqdm import tqdm  # ğŸ”¹ tqdm ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ (ì§„í–‰ë¥  í‘œì‹œ)

# ğŸ”¹ Depthwise Separable Convolution ì •ì˜
class DepthwiseSeparableConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):
        super().__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, 
                                   stride=stride, padding=kernel_size//2, groups=in_channels, bias=False)
        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU6(inplace=True)
    
    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        x = self.bn(x)
        return self.relu(x)

# ğŸ”¹ Inverted Residual Block (IRB) ì •ì˜
class InvertedResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, expansion=6):
        super().__init__()
        hidden_dim = in_channels * expansion
        self.use_residual = in_channels == out_channels
        
        self.block = nn.Sequential(
            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True),

            DepthwiseSeparableConv(hidden_dim, hidden_dim, stride=1),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True),

            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels)
        )
    
    def forward(self, x):
        if self.use_residual:
            return x + self.block(x)  # Skip Connection ì ìš©
        else:
            return self.block(x)

# ğŸ”¹ ë„¤íŠ¸ì›Œí¬ ì •ì˜
class Lip3DLandmarkNet(nn.Module):
    def __init__(self):
        super().__init__()

        # ì´ˆê¸° ë‹¤ìš´ìƒ˜í”Œë§ ë‹¨ê³„
        self.conv1 = DepthwiseSeparableConv(3, 16, stride=2)  # 256 â†’ 128
        self.conv2 = DepthwiseSeparableConv(16, 32, stride=2) # 128 â†’ 64
        self.conv3 = DepthwiseSeparableConv(32, 64, stride=2) # 64 â†’ 32
        self.conv4 = DepthwiseSeparableConv(64, 128, stride=2) # 32 â†’ 16

        # IRB ë¸”ë¡ 6ê°œ ì ìš©
        self.irb1 = InvertedResidualBlock(128, 128)
        self.irb2 = InvertedResidualBlock(128, 128)
        self.irb3 = InvertedResidualBlock(128, 128)
        self.irb4 = InvertedResidualBlock(128, 256)
        self.irb5 = InvertedResidualBlock(256, 256)
        self.irb6 = InvertedResidualBlock(256, 256)

        # Global Average Pooling & FC Layer
        self.global_pool = nn.AdaptiveAvgPool2d(1)  # (B, C, H, W) â†’ (B, C, 1, 1)
        self.fc = nn.Linear(256, 120)  # 40ê°œì˜ (x, y, z) ì¢Œí‘œë¥¼ ì¶œë ¥

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)

        x = self.irb1(x)
        x = self.irb2(x)
        x = self.irb3(x)
        x = self.irb4(x)
        x = self.irb5(x)
        x = self.irb6(x)

        x = self.global_pool(x)
        x = torch.flatten(x, 1)  # (B, 256, 1, 1) â†’ (B, 256)
        x = self.fc(x)  # (B, 120)

        return x.view(-1, 40, 3)  # (B, 40, 3) í˜•íƒœë¡œ ë³€í™˜ (x, y, z)

class Lip3DLandmarkDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform

        # ğŸ”¸ .pt íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
        self.data_list = torch.load(os.path.join(data_dir, "lips_3d_landmarks.pt"))

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        # 3ê°œì˜ ê°’ì´ ì €ì¥ëœ ê²½ìš° â†’ (ì´ë¯¸ì§€ ê²½ë¡œ, ëœë“œë§ˆí¬, ì›ë³¸ í¬ê¸°)
        img_path, landmarks, _ = self.data_list[idx]  # ì›ë³¸ í¬ê¸° (_) ë¬´ì‹œ

        img = Image.open(os.path.join(self.data_dir, img_path)).convert("RGB")

        if self.transform:
            img = self.transform(img)

        # ğŸ”¹ 3D ëœë“œë§ˆí¬ ì¢Œí‘œë¥¼ Tensorë¡œ ë³€í™˜
        landmarks = torch.tensor(landmarks, dtype=torch.float32)  # (40, 3)

        return img, landmarks
    
# ğŸ”¹ í•™ìŠµ ë£¨í”„ (ìµœì ì˜ ëª¨ë¸ ì €ì¥ & ì§„í–‰ë¥  í‘œì‹œ)
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50):
    best_val_loss = float('inf')  # ì´ˆê¸° ìµœì  ê²€ì¦ ì†ì‹¤ ì„¤ì •
    model.train()

    for epoch in range(num_epochs):
        running_loss = 0.0
        train_loader_tqdm = tqdm(train_loader, desc=f"Epoch [{epoch+1}/{num_epochs}] Training", leave=False)

        for batch_idx, (images, landmarks) in enumerate(train_loader_tqdm):
            images, landmarks = images.to(device, non_blocking=True), landmarks.to(device, non_blocking=True)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, landmarks)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            # ğŸ”¹ 10ê°œ ë°°ì¹˜ë§ˆë‹¤ í‰ê·  ì†ì‹¤ ì¶œë ¥
            if batch_idx % 10 == 0:
                avg_loss = running_loss / (batch_idx + 1)
                train_loader_tqdm.set_postfix(loss=avg_loss)

        # ğŸ”¹ ê²€ì¦ ë‹¨ê³„ (ì§„í–‰ë¥  í‘œì‹œ)
        model.eval()
        val_loss = 0.0
        val_loader_tqdm = tqdm(val_loader, desc=f"Epoch [{epoch+1}/{num_epochs}] Validation", leave=False)

        with torch.no_grad():
            for images, landmarks in val_loader_tqdm:
                images, landmarks = images.to(device, non_blocking=True), landmarks.to(device, non_blocking=True)
                outputs = model(images)
                val_loss += criterion(outputs, landmarks).item()

        val_loss /= len(val_loader)

        # ğŸ”¹ ì—í¬í¬ë³„ ì†ì‹¤ ì¶œë ¥
        print(f"\nâœ… Epoch [{epoch+1}/{num_epochs}] - Train Loss: {running_loss/len(train_loader):.6f}, Val Loss: {val_loss:.6f}")

        # ğŸ”¹ ìµœì ì˜ ëª¨ë¸ ì €ì¥ (ê²€ì¦ ì†ì‹¤ì´ ì¤„ì–´ë“¤ ë•Œë§Œ ì €ì¥)
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), "best_lip_3d_landmark_model.pth")
            print(f"âœ… ìƒˆë¡œìš´ ìµœì  ëª¨ë¸ ì €ì¥: best_lip_3d_landmark_model.pth (Val Loss: {best_val_loss:.6f})")

        model.train()

    print("âœ… ìµœì¢… í•™ìŠµ ì™„ë£Œ!")

# ğŸ”¹ ì‹¤í–‰ ì½”ë“œ
if __name__ == '__main__':
    # ğŸ”¹ ë°ì´í„° ë¡œë”©
    data_dir = r"C:\Users\User\Desktop\Lips_Landmark"
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    train_dataset = Lip3DLandmarkDataset(data_dir=data_dir, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)

    val_dataset = Lip3DLandmarkDataset(data_dir=data_dir, transform=transform)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)

    # ğŸ”¹ ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = Lip3DLandmarkNet().to(device)
    criterion = nn.SmoothL1Loss().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

    # ğŸ”¹ ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50)
